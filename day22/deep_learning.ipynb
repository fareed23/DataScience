{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Introduction to Deep Learning`\n",
    "\n",
    "- Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.\n",
    "- Deep learning is a class of machine learning algorithms that (pp199â€“200) uses multiple layers to progressively extract higher level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\n",
    "- Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
    "- Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts.\n",
    "                                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Standard Neural Network or (Artificial Neural Network)**: For structured data like excel, csv file etc.\n",
    "- **Convolutional Neural Network (CNN)**: For unstructured data like images, videos, text etc.\n",
    "- **Recurrent Neural Network (RNN)**: For data like audios/calls and languages, speech recognition etc. (sequential data - over time) one dimensional time series.\n",
    "- **Hybrid or Customised**: For data like images, Sona Info, Radar, Info, GIS Information, Videos speech, cameras scans, object detection etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear activation functions, also known as `no activation` or `identity funciton`, is where the activation is proportional to the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So essentially, a linear activation functions turns the neural network into just one layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Non-Linear Activation Functions:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Binary Step function is threshold-based.\n",
    "2. Sigmoid/Logistic function: smooth curve between 0 and 1, for one or two(binary) outputs. It is commonly used when we want to predict the probability. It is represented by an **S-shape**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Sigmoid Function makes almost no change in the prediction for very high and very low inputs which ultimately results in neural network refusing to learn further, this problem is known as the `vanishing gradient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can result in the neural network refusing to learn further, or being to slow to reach an accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TanH Function (Hyperbolic Tangent): \n",
    "- Similar to Sigmoid activation function\n",
    "- Even has the same **S-shape**\n",
    "- But the difference is output range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, tanH also comes with the *vanishing gradient* problem just like sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ReLU (Rectified Linear Unit) Function:\n",
    "- In this function, outputs for the positive inputs can range from 0 to infinity.\n",
    "- The main catch here is that ReLU function does not activate all neurons at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`f(x) = max(0, x)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since only a certain no. of neurons are activated , the ReLU is computationally efficient.\n",
    "- ReLU accelerates the convergence of gradient(change) descent towards the global minimum of the loss function due to it's linear, non-saturating property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLU: an improved version of ReLU, it add a slight slope in the negative range to prevent the dying ReLU issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`f(x) = max(0.1x, x)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric ReLU: aims to solve the problem of gradient's becoming zero for the left half of the axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`f(x) = max(ax, x)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
