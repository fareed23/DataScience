{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Introduction to Deep Learning`\n",
    "\n",
    "- Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.\n",
    "- Deep learning is a class of machine learning algorithms that (pp199â€“200) uses multiple layers to progressively extract higher level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\n",
    "- Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
    "- Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts.\n",
    "                                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Standard Neural Network or (Artificial Neural Network)**: For structured data like excel, csv file etc.\n",
    "- **Convolutional Neural Network (CNN)**: For unstructured data like images, videos, text etc.\n",
    "- **Recurrent Neural Network (RNN)**: For data like audios/calls and languages, speech recognition etc. (sequential data - over time) one dimensional time series.\n",
    "- **Hybrid or Customised**: For data like images, Sona Info, Radar, Info, GIS Information, Videos speech, cameras scans, object detection etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `activation function` in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear activation functions, also known as `no activation` or `identity funciton`, is where the activation is proportional to the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So essentially, a linear activation functions turns the neural network into just one layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Non-Linear Activation Functions:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Binary Step function is threshold-based.\n",
    "2. Sigmoid/Logistic function: smooth curve between 0 and 1, for one or two(binary) outputs. It is commonly used when we want to predict the probability. It is represented by an **S-shape**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Sigmoid Function makes almost no change in the prediction for very high and very low inputs which ultimately results in neural network refusing to learn further, this problem is known as the `vanishing gradient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can result in the neural network refusing to learn further, or being to slow to reach an accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TanH Function (Hyperbolic Tangent): \n",
    "- Similar to Sigmoid activation function\n",
    "- Even has the same **S-shape**\n",
    "- But the difference is output range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, tanH also comes with the *vanishing gradient* problem just like sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ReLU (Rectified Linear Unit) Function:\n",
    "- In this function, outputs for the positive inputs can range from 0 to infinity.\n",
    "- The main catch here is that ReLU function does not activate all neurons at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`f(x) = max(0, x)`**\n",
    "\n",
    "This means that if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since only a certain no. of neurons are activated , the ReLU is computationally efficient.\n",
    "- ReLU accelerates the convergence of gradient(change) descent towards the global minimum of the loss function due to it's linear, non-saturating property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLU: an improved version of ReLU, it add a slight slope in the negative range to prevent the dying ReLU issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`f(x) = max(0.1x, x)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric ReLU: aims to solve the problem of gradient's becoming zero for the left half of the axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`f(x) = max(ax, x)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Softamx Activation Function: \n",
    "- Generalised form of sigmoid function for multiple dimensions.\n",
    "- Converts the vector of numbers into vector of probabilities\n",
    "- Used in the case of ***multi-class classification***.\n",
    "- It is often used **exclusively for the output layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Assignments:`\n",
    "1. What are the differences between softmax and sigmoid functions?\n",
    "2. How do we know, which activation function should we use in hidden or output layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Activation for Hidden Layers`\n",
    "\n",
    "A hidden layer in a neural network is a layer that receives input from another layer (such as another hidden layer or an input layer) and provides output to another layer (such as another hidden layer or an output layer).\n",
    "\n",
    "A neural network may have zero or more hidden layers.\n",
    "\n",
    "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function.\n",
    "\n",
    "There are perhaps three activation functions you may want to consider for use in hidden layers; they are:\n",
    "1. Rectified Linear Activation (ReLU)\n",
    "2. Logistic (Sigmoid)\n",
    "3. Hyperbolic Tangent (Tanh)\n",
    "\n",
    "This is not an exhaustive list of activation functions used for hidden layers, but they are the most commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Activation for Output Layers`\n",
    "\n",
    "The output layer is the layer in a neural network model that directly outputs a prediction.\n",
    "\n",
    "All feed-forward neural network models have an output layer.\n",
    "\n",
    "There are perhaps three activation functions you may want to consider for use in the output layer; they are:\n",
    "\n",
    "1. Linear\n",
    "2. Logistic (Sigmoid)\n",
    "3. Softmax\n",
    "\n",
    "This is not an exhaustive list of activation functions used for output layers, but they are the most commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`How to Choose an Output Activation Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must choose the activation function for your output layer based on the type of prediction problem that you are solving.\n",
    "\n",
    "Specifically, the type of variable that is being predicted.\n",
    "\n",
    "For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) and predicting a numerical variable (regression).\n",
    "\n",
    "1. If your problem is a regression problem, you should use a linear activation function.\n",
    "\n",
    "- **Regression: One node, linear activation.**\n",
    "\n",
    "2. If your problem is a classification problem, then there are three main types of classification problems and each may use a different activation function.\n",
    "\n",
    "- If there are two mutually exclusive classes (binary classification), then your output layer will have one node and a **sigmoid** activation function should be used. If there are more than two mutually exclusive classes (multiclass classification), then your output layer will have one node per class and a **softmax** activation should be used. If there are two or more mutually inclusive classes (multilabel classification), then your output layer will have one node for each class and a **sigmoid** activation function is used.\n",
    "\n",
    "- **Binary Classification: One node, sigmoid activation.**\n",
    "- **Multiclass Classification: One node per class, softmax activation.**\n",
    "- **Multilabel Classification: One node per class, sigmoid activation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
